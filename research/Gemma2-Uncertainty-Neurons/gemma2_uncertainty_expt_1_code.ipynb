{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BYFbcODPO7e"
   },
   "source": [
    "# Environment Setup & Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy5ersCMQWFi"
   },
   "source": [
    "## System Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3620,
     "status": "ok",
     "timestamp": 1757717602547,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "hW7DOf11PDZG",
    "outputId": "163de8bc-f331-4282-f0ba-bd0d91b41d6a"
   },
   "outputs": [],
   "source": [
    "# æ£€æŸ¥GPUå’Œå†…å­˜çŠ¶æ€\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"=== ç³»ç»Ÿä¿¡æ¯ ===\")\n",
    "print(f\"Pythonç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "\n",
    "print(f\"ç³»ç»ŸRAM: {psutil.virtual_memory().total // 1024**3} GB\")\n",
    "print(f\"å¯ç”¨RAM: {psutil.virtual_memory().available // 1024**3} GB\")\n",
    "\n",
    "# è®¾ç½®å†…å­˜å¢é•¿ç­–ç•¥\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI62cz-LQZDZ"
   },
   "source": [
    "## Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27183,
     "status": "ok",
     "timestamp": 1757717629735,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "snmcaSJqPsFi",
    "outputId": "bcc4c415-218d-4272-e082-6270d6c3b606"
   },
   "outputs": [],
   "source": [
    "# å®‰è£…æ ¸å¿ƒåº“ (ç§»é™¤bitsandbytesï¼Œå› ä¸ºä¸éœ€è¦é‡åŒ–)\n",
    "!pip install --quiet transformers>=4.40.0\n",
    "!pip install --quiet torch>=2.0.0\n",
    "!pip install --quiet accelerate\n",
    "!pip install --quiet plotly\n",
    "!pip install --quiet numpy pandas matplotlib seaborn\n",
    "!pip install --quiet tqdm\n",
    "\n",
    "# é‡å¯è¿è¡Œæ—¶ï¼ˆè¿è¡Œå®Œè¿™ä¸ªcellåï¼Œåœ¨èœå•æ é€‰æ‹©\"è¿è¡Œæ—¶\" -> \"é‡å¯è¿è¡Œæ—¶\"ï¼‰\n",
    "print(\"å®‰è£…å®Œæˆï¼è¯·é‡å¯è¿è¡Œæ—¶ç„¶åç»§ç»­ä¸‹ä¸€æ­¥ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv24tiDzSV16"
   },
   "source": [
    "## Restart Reminder\n",
    "âš ï¸ Important: After installation, please select \"Runtime\" â†’ \"Restart session\" from the menu bar, then continue running the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DP7jDdfZSngO"
   },
   "source": [
    "## Load Gemma 2 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263,
     "referenced_widgets": [
      "af943e5689a94d99b4e14d9fab6fb441",
      "9d1086d5889543f0839b4cd5ad7f4735",
      "2f5248b2c94a41b09f8094344b80f306",
      "51321c3847cd4de1802f420ae6363c33",
      "96d4a9152a4f4611a52895cdbe4045da",
      "f3e0adbd96894b5eab19f097aaf31354",
      "618fd2b8b4f6446b97032145c448810a",
      "690b9b77f7b54bcbbfe246e01ab197de",
      "65d3426321e44edca52c6850f1494c79",
      "ab4253bd986c4a6b948ea0c2d16427d9",
      "4305d4c9e506498982f1358da9766115"
     ]
    },
    "executionInfo": {
     "elapsed": 15476,
     "status": "ok",
     "timestamp": 1757718005362,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "YufRwoxtP4kh",
    "outputId": "fba20802-b873-4b01-935d-6d6d75af118e"
   },
   "outputs": [],
   "source": [
    "# Load Gemma 2 2B in FP16\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "print(\"å¼€å§‹åŠ è½½Gemma 2 2Bæ¨¡å‹ (FP16ç²¾åº¦)...\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "try:\n",
    "    print(\"æ­£åœ¨åŠ è½½tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "\n",
    "    print(\"æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆFP16ç²¾åº¦ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-2b\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,  # ä½¿ç”¨FP16ç²¾åº¦\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,  # é™ä½CPUå†…å­˜ä½¿ç”¨\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… æ¨¡å‹æˆåŠŸåŠ è½½åˆ°è®¾å¤‡: {next(model.parameters()).device}\")\n",
    "    print(f\"æ¨¡å‹æ•°æ®ç±»å‹: {next(model.parameters()).dtype}\")\n",
    "    print(f\"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # æ£€æŸ¥æ¨¡å‹ç»“æ„\n",
    "    print(f\"æ¨¡å‹å±‚æ•°: {model.config.num_hidden_layers}\")\n",
    "    print(f\"éšè—å±‚ç»´åº¦: {model.config.hidden_size}\")\n",
    "    print(f\"è¯æ±‡è¡¨å¤§å°: {model.config.vocab_size}\")\n",
    "\n",
    "    # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() // 1024**2} MB\")\n",
    "        print(f\"GPUå†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved() // 1024**2} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "    print(\"è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•é‡æ–°è¿è¡Œæ­¤cell\")\n",
    "    print(\"å¦‚æœå†…å­˜ä¸è¶³ï¼Œè¯·è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–è€…å¯ç”¨é‡åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z08XaAdHSxzE"
   },
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1241,
     "status": "ok",
     "timestamp": 1757718008432,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "cHZxoJWDQDM_",
    "outputId": "383b4a0d-dffc-4e47-f570-4677d7cb4c00"
   },
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ¨¡å‹ç”Ÿæˆ\n",
    "test_prompt = \"The capital of France is\"\n",
    "print(f\"æµ‹è¯•æç¤º: '{test_prompt}'\")\n",
    "\n",
    "# ç¼–ç è¾“å…¥\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(f\"è¾“å…¥tokenæ•°é‡: {len(inputs['input_ids'][0])}\")\n",
    "print(f\"è¾“å…¥tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"ç”Ÿæˆç»“æœ: '{generated_text}'\")\n",
    "\n",
    "# è·å–logitsåˆ†æ\n",
    "with torch.no_grad():\n",
    "    model_outputs = model(**inputs, output_hidden_states=True)\n",
    "    logits = model_outputs.logits[0, -1]  # æœ€åä¸€ä¸ªtokençš„logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # è·å–top-5é¢„æµ‹\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "    print(\"\\n=== Top 5 é¢„æµ‹ ===\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"{i+1}. '{token}' - æ¦‚ç‡: {prob:.4f}\")\n",
    "\n",
    "    # ç†µè®¡ç®—\n",
    "    def safe_entropy_calculation(logits):\n",
    "        \"\"\"æ›´ç¨³å¥çš„ç†µè®¡ç®—æ–¹æ³•\"\"\"\n",
    "        try:\n",
    "            # æ£€æŸ¥logitsæ˜¯å¦å¼‚å¸¸\n",
    "            if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                print(\"âŒ LogitsåŒ…å«å¼‚å¸¸å€¼\")\n",
    "                return None\n",
    "\n",
    "            # ä½¿ç”¨log_softmaxé¿å…æ•°å€¼é—®é¢˜\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # æ£€æŸ¥æ¦‚ç‡æ˜¯å¦å¼‚å¸¸\n",
    "            if torch.isnan(probs).any() or torch.isinf(probs).any():\n",
    "                print(\"âŒ æ¦‚ç‡è®¡ç®—å¼‚å¸¸\")\n",
    "                return None\n",
    "\n",
    "            # è®¡ç®—ç†µ\n",
    "            entropy = -torch.sum(probs * log_probs)\n",
    "\n",
    "            if torch.isnan(entropy) or torch.isinf(entropy):\n",
    "                print(\"âŒ ç†µè®¡ç®—ç»“æœå¼‚å¸¸\")\n",
    "                return None\n",
    "\n",
    "            return entropy.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç†µè®¡ç®—é”™è¯¯: {e}\")\n",
    "            return None\n",
    "\n",
    "    entropy = safe_entropy_calculation(logits)\n",
    "    if entropy is not None:\n",
    "        print(f\"\\nâœ… é¢„æµ‹ç†µï¼ˆä¸ç¡®å®šæ€§ï¼‰: {entropy:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ ç†µè®¡ç®—å¤±è´¥\")\n",
    "        # ä½¿ç”¨æ›¿ä»£åº¦é‡\n",
    "        top1_prob = torch.max(torch.softmax(logits, dim=-1))\n",
    "        print(f\"æ›¿ä»£åº¦é‡ - Top-1æ¦‚ç‡: {top1_prob:.4f} (è¶Šé«˜è¶Šç¡®å®š)\")\n",
    "        print(f\"ä¸ç¡®å®šæ€§ä¼°è®¡: {1-top1_prob:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… æ¨¡å‹æµ‹è¯•å®Œæˆï¼Œå‡†å¤‡å¼€å§‹åˆ†æä¸ç¡®å®šæ€§ç¥ç»å…ƒï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upiLI48SFJ49"
   },
   "source": [
    "# Neuron Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRwo-HD9XIZ8"
   },
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1757718016538,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "xeEWwmPQXK_U",
    "outputId": "2402e032-1ac3-4452-ec43-6caf2d23e39c"
   },
   "outputs": [],
   "source": [
    "# ä¸ç¡®å®šæ€§ç¥ç»å…ƒæµ‹è¯•æ•°æ®é›†\n",
    "# åŸºäºä¸åŒç±»å‹çš„ä¸ç¡®å®šæ€§è®¾è®¡æµ‹è¯•å¥å­\n",
    "\n",
    "uncertainty_test_dataset = {\n",
    "\n",
    "    # 1. Epistemic Uncertainty (è®¤çŸ¥ä¸ç¡®å®šæ€§)\n",
    "    # æ¨¡å‹çŸ¥è¯†ä¸è¶³å¯¼è‡´çš„ä¸ç¡®å®šæ€§ï¼Œç†è®ºä¸Šå¯ä»¥é€šè¿‡æ›´å¤šè®­ç»ƒæ•°æ®è§£å†³\n",
    "    \"epistemic\": {\n",
    "        \"description\": \"æ¨¡å‹çŸ¥è¯†ä¸è¶³å¯¼è‡´çš„ä¸ç¡®å®šæ€§\",\n",
    "        \"expected_behavior\": \"é«˜ä¸ç¡®å®šæ€§ï¼Œæ¨¡å‹ä¸çŸ¥é“ç­”æ¡ˆ\",\n",
    "        \"sentences\": [\n",
    "            # Obscure factual knowledge\n",
    "            \"The population of Nauru in 2024 is\",\n",
    "            \"The CEO of startup company Zephyr Labs is\",\n",
    "            \"The atomic weight of Flerovium is\",\n",
    "            \"The mayor of Vaduz, Liechtenstein is\",\n",
    "            \"The 47th element on the periodic table is\",\n",
    "            \"The director of the 1927 film Metropolis was\",\n",
    "            \"The winner of the 1952 Nobel Prize in Chemistry was\",\n",
    "            \"The height of Mount Vinson in Antarctica is\",\n",
    "            # Technical/specialized knowledge\n",
    "            \"The Hausdorff dimension of the Sierpinski triangle is\",\n",
    "            \"The IUPAC name for water is\",\n",
    "            \"The half-life of Carbon-14 is\",\n",
    "            \"The speed of sound in helium at 20Â°C is\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # 2. Aleatoric Uncertainty (éšæœºä¸ç¡®å®šæ€§)\n",
    "    # è¾“å…¥æœ¬èº«å›ºæœ‰çš„æ¨¡ç³Šæ€§ï¼Œå³ä½¿æœ‰å®Œç¾æ¨¡å‹ä¹Ÿæ— æ³•ç¡®å®š\n",
    "    \"aleatoric\": {\n",
    "        \"description\": \"è¾“å…¥å›ºæœ‰çš„æ¨¡ç³Šæ€§å’Œå¤šä¹‰æ€§\",\n",
    "        \"expected_behavior\": \"ä¸­ç­‰åˆ°é«˜ä¸ç¡®å®šæ€§ï¼Œå¤šä¸ªåˆç†ç­”æ¡ˆ\",\n",
    "        \"sentences\": [\n",
    "            # Subjective/opinion-based completions\n",
    "            \"The best programming language is\",\n",
    "            \"The most important thing in life is\",\n",
    "            \"The greatest movie of all time is\",\n",
    "            \"The most beautiful color is\",\n",
    "            \"The meaning of happiness is\",\n",
    "            \"Success is defined as\",\n",
    "            # Open-ended continuations with multiple valid paths\n",
    "            \"She told him that\",\n",
    "            \"They decided to go\",\n",
    "            \"The reason for this is\",\n",
    "            \"After thinking about it,\",\n",
    "            \"The story ended when\",\n",
    "            # Lexical ambiguity (word-level multiple meanings)\n",
    "            \"The bank is\",\n",
    "            \"The bat flew\",\n",
    "            \"She couldn't bear\",\n",
    "            \"The seal swam\",\n",
    "            \"The light solution is\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # 3. Linguistic Uncertainty (è¯­è¨€ä¸ç¡®å®šæ€§)\n",
    "    # è¯­è¨€ç»“æ„æˆ–è¯­æ³•å¯¼è‡´çš„ä¸ç¡®å®šæ€§\n",
    "    \"linguistic\": {\n",
    "        \"description\": \"è¯­è¨€ç»“æ„å’Œè¯­æ³•å¯¼è‡´çš„ä¸ç¡®å®šæ€§\",\n",
    "        \"expected_behavior\": \"ç»“æ„æ€§ä¸ç¡®å®šæ€§ï¼Œè¯­æ³•è§£æå›°éš¾\",\n",
    "        \"sentences\": [\n",
    "            # PP-attachment ambiguity\n",
    "            \"The man saw the boy with the telescope\",\n",
    "            \"She hit the man with the umbrella\",\n",
    "            \"They discussed the plan in the office\",\n",
    "            \"I saw the Grand Canyon flying to New York\",\n",
    "            # Syntactic ambiguity\n",
    "            \"Flying planes can be dangerous\",\n",
    "            \"They are hunting dogs\",\n",
    "            \"Visiting relatives can be boring\",\n",
    "            \"The shooting of the hunters was terrible\",\n",
    "            # Garden path sentences\n",
    "            \"The horse raced past the barn fell\",\n",
    "            \"The old man the boats\",\n",
    "            \"The complex houses married and single soldiers\",\n",
    "            \"The prime number few\",\n",
    "            # Coordination ambiguity\n",
    "            \"Old men and women were served first\",\n",
    "            \"I saw her duck and cover\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # 4. Low Uncertainty Controls (ä½ä¸ç¡®å®šæ€§å¯¹ç…§ç»„)\n",
    "    # ç¡®å®šæ€§é«˜çš„å¥å­ï¼Œä½œä¸ºåŸºçº¿å¯¹ç…§\n",
    "    \"low_uncertainty\": {\n",
    "        \"description\": \"é«˜ç¡®å®šæ€§å¥å­ï¼Œä½œä¸ºåŸºçº¿å¯¹ç…§\",\n",
    "        \"expected_behavior\": \"ä½ä¸ç¡®å®šæ€§ï¼Œæ˜ç¡®çš„é¢„æœŸç­”æ¡ˆ\",\n",
    "        \"sentences\": [\n",
    "            # Basic facts\n",
    "            \"The capital of USA is\",\n",
    "            \"Two plus two equals\",\n",
    "            \"The sun rises in the\",\n",
    "            \"The first letter of the alphabet is\",\n",
    "            \"Christmas is celebrated on December\",\n",
    "            # Common knowledge\n",
    "            \"The color of grass is\",\n",
    "            \"The Earth orbits the\",\n",
    "            \"The opposite of hot is\",\n",
    "            \"One meter equals one hundred\"\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "def print_dataset_summary():\n",
    "    \"\"\"æ‰“å°æ•°æ®é›†æ‘˜è¦\"\"\"\n",
    "    total_sentences = 0\n",
    "    print(\"=== ä¸ç¡®å®šæ€§æµ‹è¯•æ•°æ®é›†æ‘˜è¦ ===\\n\")\n",
    "\n",
    "    for category, data in uncertainty_test_dataset.items():\n",
    "        num_sentences = len(data[\"sentences\"])\n",
    "        total_sentences += num_sentences\n",
    "\n",
    "        print(f\"ğŸ“Š {category.upper().replace('_', ' ')} ({num_sentences} å¥å­)\")\n",
    "        print(f\"   æè¿°: {data['description']}\")\n",
    "        print(f\"   é¢„æœŸè¡Œä¸º: {data['expected_behavior']}\")\n",
    "        print(\"   ç¤ºä¾‹å¥å­:\")\n",
    "        for i, sentence in enumerate(data[\"sentences\"][:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª\n",
    "            print(f\"     â€¢ '{sentence}'\")\n",
    "        if num_sentences > 2:\n",
    "            print(f\"     ... è¿˜æœ‰ {num_sentences - 2} ä¸ªå¥å­\")\n",
    "        print()\n",
    "\n",
    "    print(f\"ğŸ“ˆ æ€»è®¡: {total_sentences} ä¸ªæµ‹è¯•å¥å­\")\n",
    "    print(f\"â±ï¸  é¢„è®¡å®éªŒæ—¶é—´: {total_sentences * 0.5:.1f}-{total_sentences * 1:.1f} åˆ†é’Ÿ\")\n",
    "    return total_sentences\n",
    "\n",
    "def get_test_sentences_by_category(category=None, limit_per_category=None):\n",
    "    \"\"\"\n",
    "    è·å–æŒ‡å®šç±»åˆ«çš„æµ‹è¯•å¥å­\n",
    "\n",
    "    Args:\n",
    "        category: æŒ‡å®šç±»åˆ«ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨\n",
    "        limit_per_category: æ¯ä¸ªç±»åˆ«çš„å¥å­æ•°é‡é™åˆ¶\n",
    "    \"\"\"\n",
    "    if category and category in uncertainty_test_dataset:\n",
    "        sentences = uncertainty_test_dataset[category][\"sentences\"]\n",
    "        if limit_per_category:\n",
    "            sentences = sentences[:limit_per_category]\n",
    "        return [(sentence, category) for sentence in sentences]\n",
    "\n",
    "    # è¿”å›æ‰€æœ‰ç±»åˆ«\n",
    "    all_sentences = []\n",
    "    for cat, data in uncertainty_test_dataset.items():\n",
    "        sentences = data[\"sentences\"]\n",
    "        if limit_per_category:\n",
    "            sentences = sentences[:limit_per_category]\n",
    "        all_sentences.extend([(sentence, cat) for sentence in sentences])\n",
    "\n",
    "    return all_sentences\n",
    "\n",
    "def get_recommended_test_set(quick_test=True):\n",
    "    \"\"\"\n",
    "    è·å–æ¨èçš„æµ‹è¯•é›†\n",
    "\n",
    "    Args:\n",
    "        quick_test: True = å¿«é€Ÿæµ‹è¯•(æ¯ç±»3-4å¥), False = å®Œæ•´æµ‹è¯•\n",
    "    \"\"\"\n",
    "    if quick_test:\n",
    "        print(\"ğŸš€ æ¨èï¼šå¿«é€Ÿæµ‹è¯•é›† (æ¯ç±»3-4ä¸ªå¥å­ï¼Œæ€»è®¡çº¦15ä¸ª)\")\n",
    "        limit = 4\n",
    "    else:\n",
    "        print(\"ğŸ”¬ æ¨èï¼šå®Œæ•´æµ‹è¯•é›† (æ‰€æœ‰å¥å­)\")\n",
    "        limit = None\n",
    "\n",
    "    return get_test_sentences_by_category(limit_per_category=limit)\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯\n",
    "total_count = print_dataset_summary()\n",
    "\n",
    "# æä¾›ä½¿ç”¨å»ºè®®\n",
    "print(\"ğŸ’¡ ä½¿ç”¨å»ºè®®:\")\n",
    "print(\"   â€¢ ç¬¬ä¸€æ¬¡å®éªŒï¼šä½¿ç”¨ get_recommended_test_set(quick_test=True)\")\n",
    "print(\"   â€¢ è¯¦ç»†åˆ†æï¼šä½¿ç”¨ get_recommended_test_set(quick_test=False)\")\n",
    "print(\"   â€¢ ç‰¹å®šåˆ†æï¼šä½¿ç”¨ get_test_sentences_by_category('epistemic')\")\n",
    "print(\"\\nâœ… æµ‹è¯•æ•°æ®é›†å‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnOSDG5kFmSJ"
   },
   "source": [
    "## Weight Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1757718030243,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "dbwQIjYPXX3R",
    "outputId": "03f50f6d-8308-4bb0-c055-540a08eecfa9"
   },
   "outputs": [],
   "source": [
    "print(\"å¼€å§‹æå–æœ€åä¸€å±‚ç¥ç»å…ƒæƒé‡...\")\n",
    "\n",
    "# è·å–æ¨¡å‹çš„æœ€åä¸€å±‚\n",
    "last_layer_idx = model.config.num_hidden_layers - 1\n",
    "print(f\"åˆ†æç¬¬{last_layer_idx}å±‚ï¼ˆæœ€åä¸€å±‚ï¼‰\")\n",
    "\n",
    "# æå–æœ€åä¸€å±‚çš„è¾“å‡ºæƒé‡å’ŒunembeddingçŸ©é˜µ\n",
    "try:\n",
    "    # Gemma 2çš„ç»“æ„è®¿é—®\n",
    "    last_layer = model.model.layers[last_layer_idx]\n",
    "\n",
    "    # è·å–MLPçš„è¾“å‡ºæƒé‡ (hidden_size, intermediate_size)\n",
    "    mlp_gate_proj = last_layer.mlp.gate_proj.weight.data  # (intermediate_size, hidden_size)\n",
    "    mlp_up_proj = last_layer.mlp.up_proj.weight.data      # (intermediate_size, hidden_size)\n",
    "    mlp_down_proj = last_layer.mlp.down_proj.weight.data  # (hidden_size, intermediate_size)\n",
    "\n",
    "    # è·å–unembeddingçŸ©é˜µ\n",
    "    unembed_matrix = model.lm_head.weight.data  # (vocab_size, hidden_size)\n",
    "\n",
    "    print(f\"MLP gate projectionå½¢çŠ¶: {mlp_gate_proj.shape}\")\n",
    "    print(f\"MLP up projectionå½¢çŠ¶: {mlp_up_proj.shape}\")\n",
    "    print(f\"MLP down projectionå½¢çŠ¶: {mlp_down_proj.shape}\")\n",
    "    print(f\"UnembeddingçŸ©é˜µå½¢çŠ¶: {unembed_matrix.shape}\")\n",
    "\n",
    "    # è®¡ç®—æœ‰æ•ˆçš„è¾“å‡ºæƒé‡ (æˆ‘ä»¬å…³æ³¨down_projï¼Œå®ƒæ˜¯MLPçš„è¾“å‡º)\n",
    "    W_out = mlp_down_proj.T  # è½¬ç½®ä¸º (intermediate_size, hidden_size)\n",
    "    print(f\"è¾“å‡ºæƒé‡çŸ©é˜µå½¢çŠ¶: {W_out.shape}\")\n",
    "    print(f\"è®¾å¤‡: {W_out.device}\")\n",
    "    print(f\"æ•°æ®ç±»å‹: {W_out.dtype}\")\n",
    "\n",
    "    # è½¬æ¢ä¸ºCPUè¿›è¡Œåˆ†æï¼Œä¿æŒFP16ç²¾åº¦\n",
    "    W_out_cpu = W_out.cpu()  # ä¿æŒFP16\n",
    "    unembed_cpu = unembed_matrix.cpu()  # ä¿æŒFP16\n",
    "\n",
    "    print(\"âœ… æƒé‡æå–å®Œæˆï¼\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æƒé‡æå–å¤±è´¥: {e}\")\n",
    "    print(\"æ¨¡å‹ç»“æ„å¯èƒ½ä¸é¢„æœŸä¸åŒï¼Œè®©æˆ‘ä»¬æ£€æŸ¥å®é™…ç»“æ„...\")\n",
    "\n",
    "    # æ‰“å°æ¨¡å‹ç»“æ„ä»¥ä¾¿è°ƒè¯•\n",
    "    print(\"\\n=== æ¨¡å‹ç»“æ„æ£€æŸ¥ ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        if 'layer' in name and 'mlp' in name:\n",
    "            print(f\"{name}: {type(module)}\")\n",
    "            if hasattr(module, 'weight'):\n",
    "                print(f\"  æƒé‡å½¢çŠ¶: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfzgwtF2FnZ1"
   },
   "source": [
    "## LogitVar Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26336,
     "status": "ok",
     "timestamp": 1757719108689,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "gHV5gOzuXawq",
    "outputId": "e98ffe42-1313-495d-a634-e1827b603215"
   },
   "outputs": [],
   "source": [
    "# è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„LogitVarå’Œæƒé‡èŒƒæ•°\n",
    "print(\"è®¡ç®—LogitVaræŒ‡æ ‡...\")\n",
    "\n",
    "def calculate_logit_var(neuron_weights, unembed_matrix):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç¥ç»å…ƒçš„LogitVaræŒ‡æ ‡\n",
    "    LogitVar(i) = Var(W_out^(i) @ W_U / ||W_out^(i) @ W_U||_2)\n",
    "    \"\"\"\n",
    "    # ç¡®ä¿è®¡ç®—ç²¾åº¦ï¼Œè½¬æ¢ä¸ºfloat32è¿›è¡Œæ•°å€¼è®¡ç®—\n",
    "    neuron_weights_f32 = neuron_weights.float()\n",
    "    unembed_matrix_f32 = unembed_matrix.float()\n",
    "\n",
    "    # è®¡ç®—ç¥ç»å…ƒæƒé‡ä¸unembeddingçš„ä¹˜ç§¯\n",
    "    projections = neuron_weights_f32 @ unembed_matrix_f32.T\n",
    "    norms = torch.norm(projections, dim=1, keepdim=True)\n",
    "\n",
    "    # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒæŠ•å½±çš„L2èŒƒæ•° æ ‡å‡†åŒ–æŠ•å½±\n",
    "    norms = torch.clamp(norms, min=1e-10)  # å¢åŠ æœ€å°å€¼\n",
    "    normalized_projections = projections / norms\n",
    "\n",
    "    # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒæ ‡å‡†åŒ–æŠ•å½±çš„æ–¹å·®\n",
    "    mask = torch.isfinite(normalized_projections).all(dim=1)\n",
    "    logit_vars = torch.full((normalized_projections.shape[0],), float('nan'))\n",
    "    logit_vars[mask] = torch.var(normalized_projections[mask], dim=1)\n",
    "\n",
    "    return logit_vars, norms.squeeze()\n",
    "\n",
    "# æ‰§è¡Œè®¡ç®—\n",
    "try:\n",
    "    print(f\"è®¡ç®—{W_out_cpu.shape[0]}ä¸ªç¥ç»å…ƒçš„æŒ‡æ ‡...\")\n",
    "\n",
    "    logit_vars, projection_norms = calculate_logit_var(W_out_cpu, unembed_cpu)\n",
    "\n",
    "    # è®¡ç®—è¾“å‡ºæƒé‡çš„L2èŒƒæ•°\n",
    "    weight_norms = torch.norm(W_out_cpu.float(), dim=1)  # æ¯ä¸ªç¥ç»å…ƒæƒé‡å‘é‡çš„èŒƒæ•°\n",
    "\n",
    "    print(f\"LogitVarè®¡ç®—å®Œæˆ: {logit_vars.shape}\")\n",
    "    print(f\"æƒé‡èŒƒæ•°è®¡ç®—å®Œæˆ: {weight_norms.shape}\")\n",
    "    print(f\"æŠ•å½±èŒƒæ•°è®¡ç®—å®Œæˆ: {projection_norms.shape}\")\n",
    "\n",
    "    # åŸºæœ¬ç»Ÿè®¡\n",
    "    print(f\"\\n=== ç»Ÿè®¡æ‘˜è¦ ===\")\n",
    "    print(f\"LogitVar - å‡å€¼: {logit_vars.mean():.6f}, æ ‡å‡†å·®: {logit_vars.std():.6f}\")\n",
    "    print(f\"æƒé‡èŒƒæ•° - å‡å€¼: {weight_norms.mean():.6f}, æ ‡å‡†å·®: {weight_norms.std():.6f}\")\n",
    "    print(f\"æŠ•å½±èŒƒæ•° - å‡å€¼: {projection_norms.mean():.6f}, æ ‡å‡†å·®: {projection_norms.std():.6f}\")\n",
    "\n",
    "    # å¯»æ‰¾å¼‚å¸¸å€¼ï¼ˆä½LogitVarä½†é«˜æƒé‡èŒƒæ•°çš„ç¥ç»å…ƒï¼‰\n",
    "    logit_var_threshold = logit_vars.quantile(0.1)  # æœ€ä½10%çš„LogitVar\n",
    "    weight_norm_threshold = weight_norms.quantile(0.9)  # æœ€é«˜10%çš„æƒé‡èŒƒæ•°\n",
    "\n",
    "    # å€™é€‰ä¸ç¡®å®šæ€§ç¥ç»å…ƒ\n",
    "    uncertainty_candidates = (logit_vars < logit_var_threshold) & (weight_norms > weight_norm_threshold)\n",
    "    num_candidates = uncertainty_candidates.sum().item()\n",
    "\n",
    "    print(f\"\\n=== å€™é€‰ä¸ç¡®å®šæ€§ç¥ç»å…ƒ ===\")\n",
    "    print(f\"ä½LogitVaré˜ˆå€¼: {logit_var_threshold:.6f}\")\n",
    "    print(f\"é«˜æƒé‡èŒƒæ•°é˜ˆå€¼: {weight_norm_threshold:.6f}\")\n",
    "    print(f\"æ‰¾åˆ°å€™é€‰ç¥ç»å…ƒ: {num_candidates} ä¸ª\")\n",
    "\n",
    "    if num_candidates > 0:\n",
    "        candidate_indices = torch.where(uncertainty_candidates)[0]\n",
    "        print(f\"å€™é€‰ç¥ç»å…ƒç´¢å¼•: {candidate_indices.tolist()}\")\n",
    "\n",
    "        # æ˜¾ç¤ºå‰5ä¸ªå€™é€‰ç¥ç»å…ƒçš„è¯¦ç»†ä¿¡æ¯\n",
    "        for i, idx in enumerate(candidate_indices[:5]):\n",
    "            print(f\"  ç¥ç»å…ƒ {idx.item()}: LogitVar={logit_vars[idx]:.6f}, æƒé‡èŒƒæ•°={weight_norms[idx]:.6f}\")\n",
    "\n",
    "    print(\"\\nâœ… LogitVaråˆ†æå®Œæˆï¼\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è®¡ç®—å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybNIjtr3Fpsg"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1273,
     "status": "ok",
     "timestamp": 1757723969627,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "cdk0IYL0FrfT",
    "outputId": "aa18f445-126f-42bf-db6b-a921f4fc7280"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Gemma 2 2B (FP16) Last Layer Neuron Analysis: Finding Uncertainty Neurons', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Main scatter plot: Weight norm vs LogitVar\n",
    "ax1 = axes[0, 0]\n",
    "scatter = ax1.scatter(weight_norms, logit_vars, alpha=0.6, s=30, c='blue', edgecolors='none')\n",
    "ax1.set_xlabel('Weight L2 Norm', fontsize=12)\n",
    "ax1.set_ylabel('LogitVar', fontsize=12)\n",
    "ax1.set_title('Weight Norm vs LogitVar\\n(Bottom-right = Uncertainty Neuron Candidates)', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark candidate uncertainty neurons\n",
    "if num_candidates > 0:\n",
    "    candidate_x = weight_norms[uncertainty_candidates]\n",
    "    candidate_y = logit_vars[uncertainty_candidates]\n",
    "    ax1.scatter(candidate_x, candidate_y, c='red', s=80, marker='o',\n",
    "               edgecolors='black', linewidths=2, alpha=0.8, label=f'Candidates ({num_candidates} neurons)')\n",
    "    ax1.legend()\n",
    "\n",
    "# 2. LogitVar distribution histogram\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(logit_vars.numpy(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "ax2.axvline(logit_var_threshold, color='red', linestyle='--', linewidth=2, label=f'10th percentile: {logit_var_threshold:.4f}')\n",
    "ax2.set_xlabel('LogitVar', fontsize=12)\n",
    "ax2.set_ylabel('Number of Neurons', fontsize=12)\n",
    "ax2.set_title('LogitVar Distribution', fontsize=11)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Weight norm distribution histogram\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(weight_norms.numpy(), bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "ax3.axvline(weight_norm_threshold, color='red', linestyle='--', linewidth=2, label=f'90th percentile: {weight_norm_threshold:.4f}')\n",
    "ax3.set_xlabel('Weight L2 Norm', fontsize=12)\n",
    "ax3.set_ylabel('Number of Neurons', fontsize=12)\n",
    "ax3.set_title('Weight Norm Distribution', fontsize=11)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Projection norm vs LogitVar\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(projection_norms, logit_vars, alpha=0.6, s=30, c='purple', edgecolors='none')\n",
    "ax4.set_xlabel('Projection Norm (||W_out @ W_U||)', fontsize=12)\n",
    "ax4.set_ylabel('LogitVar', fontsize=12)\n",
    "ax4.set_title('Projection Norm vs LogitVar', fontsize=11)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "if num_candidates > 0:\n",
    "    candidate_proj = projection_norms[uncertainty_candidates]\n",
    "    ax4.scatter(candidate_proj, candidate_y, c='red', s=80, marker='o',\n",
    "               edgecolors='black', linewidths=2, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis results\n",
    "print(\"=== Detailed Analysis Results ===\")\n",
    "print(f\"Total number of neurons: {len(weight_norms)}\")\n",
    "print(f\"Candidate uncertainty neurons: {num_candidates}\")\n",
    "\n",
    "if num_candidates > 0:\n",
    "    print(f\"\\n=== Top {min(30, num_candidates)} Candidate Neuron Details ===\")\n",
    "    candidate_indices = torch.where(uncertainty_candidates)[0]\n",
    "    for i, idx in enumerate(candidate_indices[:min(30, num_candidates)]):\n",
    "        idx_val = idx.item()\n",
    "        print(f\"Neuron {idx_val:4d}: \"\n",
    "              f\"LogitVar={logit_vars[idx]:.4e}, \"\n",
    "              f\"Weight norm={weight_norms[idx]:.4f}, \"\n",
    "              f\"Projection norm={projection_norms[idx]:.4f}\")\n",
    "\n",
    "    # Save candidate neuron indices for subsequent analysis\n",
    "    top_candidates = candidate_indices[:5] if num_candidates >= 5 else candidate_indices\n",
    "    print(f\"\\nSelecting top {len(top_candidates)} neurons for further validation: {top_candidates.tolist()}\")\n",
    "else:\n",
    "    print(\"No obvious candidate uncertainty neurons found\")\n",
    "    # Select some boundary cases for analysis\n",
    "    sorted_indices = torch.argsort(logit_vars)\n",
    "    top_candidates = sorted_indices[:3]  # Top 3 neurons with lowest LogitVar\n",
    "    print(f\"Selecting 3 neurons with lowest LogitVar for analysis: {top_candidates.tolist()}\")\n",
    "\n",
    "print(\"\\nâœ… Visualization analysis completed! Next we will validate the causal effects of these candidate neurons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWxOF4CpFTm0"
   },
   "source": [
    "# Causal Inference Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXxad-ZMGGuB"
   },
   "source": [
    "## Causal Verification Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1757722301515,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "3kGAKxJyYzkM",
    "outputId": "9647d051-7a1c-4a91-9fa3-737f41af9526"
   },
   "outputs": [],
   "source": [
    "# å› æœéªŒè¯ï¼šæ¿€æ´»è¡¥ä¸å®éªŒ\n",
    "print(\"å‡†å¤‡å› æœéªŒè¯å®éªŒ...\")\n",
    "\n",
    "def calculate_entropy(logits):\n",
    "    \"\"\"è®¡ç®—é¢„æµ‹çš„ç†µï¼ˆä¸ç¡®å®šæ€§åº¦é‡ï¼‰\"\"\"\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "    return entropy\n",
    "\n",
    "def create_hook_fn(layer_idx, neuron_indices, intervention_type='zero'):\n",
    "    \"\"\"åˆ›å»ºæ¿€æ´»å¹²é¢„çš„hookå‡½æ•°\"\"\"\n",
    "    def hook_fn(module, input, output):\n",
    "        # Gemma 2çš„MLPè¾“å‡ºé€šå¸¸æ˜¯hidden states\n",
    "        if intervention_type == 'zero':\n",
    "            # å°†æŒ‡å®šç¥ç»å…ƒçš„æ¿€æ´»è®¾ä¸º0\n",
    "            for neuron_idx in neuron_indices:\n",
    "                if neuron_idx < output.shape[-1]:\n",
    "                    output[:, :, neuron_idx] = 0\n",
    "        elif intervention_type == 'mean':\n",
    "            # å°†æŒ‡å®šç¥ç»å…ƒçš„æ¿€æ´»è®¾ä¸ºè¯¥å±‚çš„å‡å€¼\n",
    "            layer_mean = output.mean(dim=[0, 1], keepdim=True)\n",
    "            for neuron_idx in neuron_indices:\n",
    "                if neuron_idx < output.shape[-1]:\n",
    "                    output[:, :, neuron_idx] = layer_mean[:, :, neuron_idx]\n",
    "        return output\n",
    "    return hook_fn\n",
    "\n",
    "# ä½¿ç”¨ç»“æ„åŒ–çš„æµ‹è¯•æ•°æ®é›†\n",
    "print(\"=== é€‰æ‹©æµ‹è¯•å¥å­ ===\")\n",
    "\n",
    "# ä½¿ç”¨æ›´å¤§æµ‹è¯•é›†\n",
    "test_data = get_recommended_test_set(quick_test=False)\n",
    "test_sentences = [item[0] for item in test_data]  # æå–å¥å­\n",
    "sentence_categories = [item[1] for item in test_data]  # æå–ç±»åˆ«\n",
    "\n",
    "print(f\"é€‰æ‹©äº† {len(test_sentences)} ä¸ªæµ‹è¯•å¥å­ï¼Œæ¶µç›– {len(set(sentence_categories))} ç§ä¸ç¡®å®šæ€§ç±»å‹\")\n",
    "print(\"\\næŒ‰ç±»åˆ«æ˜¾ç¤ºæµ‹è¯•å¥å­:\")\n",
    "\n",
    "# æŒ‰ç±»åˆ«ç»„ç»‡æ˜¾ç¤º\n",
    "for category in set(sentence_categories):\n",
    "    category_sentences = [sent for sent, cat in test_data if cat == category]\n",
    "    print(f\"\\nğŸ“Š {category.upper().replace('_', ' ')} ({len(category_sentences)} å¥å­):\")\n",
    "    for i, sentence in enumerate(category_sentences):\n",
    "        print(f\"   {i+1}. '{sentence}'\")\n",
    "\n",
    "# é€‰æ‹©è¦æµ‹è¯•çš„ç¥ç»å…ƒ\n",
    "if 'top_candidates' in locals() and len(top_candidates) > 0:\n",
    "    test_neurons = top_candidates[:5].tolist()  # æµ‹è¯•å‰5ä¸ªå€™é€‰ç¥ç»å…ƒ\n",
    "    print(f\"\\nå°†æµ‹è¯•ç¥ç»å…ƒ: {test_neurons}\")\n",
    "else:\n",
    "    # å¦‚æœæ²¡æœ‰æ˜æ˜¾å€™é€‰ï¼Œéšæœºé€‰æ‹©ä¸€äº›ç¥ç»å…ƒä½œä¸ºå¯¹ç…§\n",
    "    test_neurons = [100, 200, 500]  # ç¤ºä¾‹ç¥ç»å…ƒç´¢å¼•\n",
    "    print(f\"\\nä½¿ç”¨ç¤ºä¾‹ç¥ç»å…ƒè¿›è¡Œæµ‹è¯•: {test_neurons}\")\n",
    "\n",
    "print(\"\\nâœ… éªŒè¯å®éªŒè®¾ç½®å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQvheHSNGKRJ"
   },
   "source": [
    "## Activation Patching Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1757722308768,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "7bpaMA7gY2nQ",
    "outputId": "8a0e787b-4297-4331-9ba6-d8a90e864efb"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from contextlib import contextmanager\n",
    "\n",
    "print(\"å¼€å§‹æ‰§è¡Œæ¿€æ´»è¡¥ä¸å®éªŒ...\")\n",
    "\n",
    "def run_intervention_experiment(model, tokenizer, test_data, neuron_indices, layer_idx):\n",
    "    \"\"\"\n",
    "    è¿è¡Œç¥ç»å…ƒå¹²é¢„å®éªŒ\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'sentences': [],\n",
    "        'categories': [],\n",
    "        'baseline_entropy': [],\n",
    "        'zero_entropy': [],\n",
    "        'mean_entropy': [],\n",
    "        'entropy_change_zero': [],\n",
    "        'entropy_change_mean': [],\n",
    "        'baseline_top_tokens': [],\n",
    "        'zero_top_tokens': [],\n",
    "        'mean_top_tokens': []\n",
    "    }\n",
    "\n",
    "    # è·å–è¦å¹²é¢„çš„å±‚\n",
    "    target_layer = model.model.layers[layer_idx].mlp\n",
    "\n",
    "    for sentence, category in tqdm(test_data, desc=\"æµ‹è¯•å¥å­\"):\n",
    "        # ç¼–ç è¾“å…¥\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # 1. åŸºçº¿é¢„æµ‹ï¼ˆæ— å¹²é¢„ï¼‰\n",
    "        with torch.no_grad():\n",
    "            baseline_outputs = model(**inputs)\n",
    "            baseline_logits = baseline_outputs.logits[0, -1]  # æœ€åä¸€ä¸ªtokençš„logits\n",
    "            baseline_entropy = calculate_entropy(baseline_logits).item()\n",
    "\n",
    "            # è·å–top-3é¢„æµ‹\n",
    "            baseline_probs = torch.softmax(baseline_logits, dim=-1)\n",
    "            baseline_top_probs, baseline_top_indices = torch.topk(baseline_probs, 3)\n",
    "            baseline_top_tokens = [tokenizer.decode([idx]).strip() for idx in baseline_top_indices]\n",
    "\n",
    "        # 2. é›¶åŒ–å¹²é¢„\n",
    "        zero_hook = target_layer.register_forward_hook(\n",
    "            create_hook_fn(layer_idx, neuron_indices, 'zero')\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            zero_outputs = model(**inputs)\n",
    "            zero_logits = zero_outputs.logits[0, -1]\n",
    "            zero_entropy = calculate_entropy(zero_logits).item()\n",
    "\n",
    "            zero_probs = torch.softmax(zero_logits, dim=-1)\n",
    "            zero_top_probs, zero_top_indices = torch.topk(zero_probs, 3)\n",
    "            zero_top_tokens = [tokenizer.decode([idx]).strip() for idx in zero_top_indices]\n",
    "\n",
    "        zero_hook.remove()\n",
    "\n",
    "        # 3. å‡å€¼å¹²é¢„\n",
    "        mean_hook = target_layer.register_forward_hook(\n",
    "            create_hook_fn(layer_idx, neuron_indices, 'mean')\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean_outputs = model(**inputs)\n",
    "            mean_logits = mean_outputs.logits[0, -1]\n",
    "            mean_entropy = calculate_entropy(mean_logits).item()\n",
    "\n",
    "            mean_probs = torch.softmax(mean_logits, dim=-1)\n",
    "            mean_top_probs, mean_top_indices = torch.topk(mean_probs, 3)\n",
    "            mean_top_tokens = [tokenizer.decode([idx]).strip() for idx in mean_top_indices]\n",
    "\n",
    "        mean_hook.remove()\n",
    "\n",
    "        # è®¡ç®—ç†µå˜åŒ–\n",
    "        entropy_change_zero = zero_entropy - baseline_entropy\n",
    "        entropy_change_mean = mean_entropy - baseline_entropy\n",
    "\n",
    "        # ä¿å­˜ç»“æœ\n",
    "        results['sentences'].append(sentence)\n",
    "        results['categories'].append(category)\n",
    "        results['baseline_entropy'].append(baseline_entropy)\n",
    "        results['zero_entropy'].append(zero_entropy)\n",
    "        results['mean_entropy'].append(mean_entropy)\n",
    "        results['entropy_change_zero'].append(entropy_change_zero)\n",
    "        results['entropy_change_mean'].append(entropy_change_mean)\n",
    "        results['baseline_top_tokens'].append(baseline_top_tokens)\n",
    "        results['zero_top_tokens'].append(zero_top_tokens)\n",
    "        results['mean_top_tokens'].append(mean_top_tokens)\n",
    "\n",
    "        # å®æ—¶æ˜¾ç¤ºç»“æœ\n",
    "        print(f\"\\n--- [{category.upper()}] '{sentence}' ---\")\n",
    "        print(f\"åŸºçº¿ç†µ: {baseline_entropy:.4f}\")\n",
    "        print(f\"é›¶åŒ–ç†µ: {zero_entropy:.4f} (Î”: {entropy_change_zero:+.4f})\")\n",
    "        print(f\"å‡å€¼ç†µ: {mean_entropy:.4f} (Î”: {entropy_change_mean:+.4f})\")\n",
    "        print(f\"Top-3é¢„æµ‹: {' | '.join(baseline_top_tokens)}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F94ppKGQGRQM"
   },
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7654,
     "status": "ok",
     "timestamp": 1757722324063,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "LFekZGZAY48j",
    "outputId": "6bfac958-0482-4099-8d4d-83c00a078dde"
   },
   "outputs": [],
   "source": [
    "# Execute complete uncertainty neuron validation experiment\n",
    "print(\"ğŸ§ª Starting uncertainty neuron validation experiment\\n\")\n",
    "\n",
    "# Set experiment parameters\n",
    "LAYER_IDX = last_layer_idx  # Use the last layer\n",
    "print(f\"Target layer: Layer {LAYER_IDX}\")\n",
    "print(f\"Test neurons: {test_neurons}\")\n",
    "print(f\"Number of test sentences: {len(test_sentences)}\")\n",
    "\n",
    "# Execute experiment\n",
    "experiment_results = run_intervention_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_data=test_data,\n",
    "    neuron_indices=test_neurons,\n",
    "    layer_idx=LAYER_IDX\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Experiment completed! Tested {len(experiment_results['sentences'])} sentences\")\n",
    "\n",
    "# === Results Analysis ===\n",
    "def analyze_results_by_category(results):\n",
    "    \"\"\"Analyze experiment results by uncertainty type\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame({\n",
    "        'sentence': results['sentences'],\n",
    "        'category': results['categories'],\n",
    "        'baseline_entropy': results['baseline_entropy'],\n",
    "        'entropy_change_zero': results['entropy_change_zero'],\n",
    "        'entropy_change_mean': results['entropy_change_mean']\n",
    "    })\n",
    "\n",
    "    print(\"=== Results Analysis by Uncertainty Type ===\\n\")\n",
    "\n",
    "    # Statistics by category\n",
    "    category_stats = df.groupby('category').agg({\n",
    "        'baseline_entropy': ['mean', 'std', 'count'],\n",
    "        'entropy_change_zero': ['mean', 'std'],\n",
    "        'entropy_change_mean': ['mean', 'std']\n",
    "    }).round(4)\n",
    "\n",
    "    for category in df['category'].unique():\n",
    "        cat_data = df[df['category'] == category]\n",
    "        n_samples = len(cat_data)\n",
    "\n",
    "        print(f\"ğŸ“Š {category.upper().replace('_', ' ')} ({n_samples} samples)\")\n",
    "        print(f\"   Baseline entropy: {cat_data['baseline_entropy'].mean():.4f} Â± {cat_data['baseline_entropy'].std():.4f}\")\n",
    "        print(f\"   Zero intervention effect: {cat_data['entropy_change_zero'].mean():.4f} Â± {cat_data['entropy_change_zero'].std():.4f}\")\n",
    "        print(f\"   Mean intervention effect: {cat_data['entropy_change_mean'].mean():.4f} Â± {cat_data['entropy_change_mean'].std():.4f}\")\n",
    "\n",
    "        # Effect direction analysis\n",
    "        zero_positive = (cat_data['entropy_change_zero'] > 0).sum()\n",
    "        mean_positive = (cat_data['entropy_change_mean'] > 0).sum()\n",
    "        print(f\"   Proportion with zero intervention increasing entropy: {zero_positive/n_samples:.1%}\")\n",
    "        print(f\"   Proportion with mean intervention increasing entropy: {mean_positive/n_samples:.1%}\")\n",
    "        print()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Execute analysis\n",
    "results_df = analyze_results_by_category(experiment_results)\n",
    "\n",
    "# === Visualize Analysis Results ===\n",
    "def plot_experiment_results(df):\n",
    "    \"\"\"Visualize experiment results\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Uncertainty Neuron Intervention Experiment Results Analysis (FP16)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Baseline entropy distribution by category\n",
    "    ax1 = axes[0, 0]\n",
    "    categories = df['category'].unique()\n",
    "    colors = sns.color_palette(\"husl\", len(categories))\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        cat_data = df[df['category'] == category]\n",
    "        ax1.scatter(cat_data.index, cat_data['baseline_entropy'],\n",
    "                   label=category.replace('_', ' '), alpha=0.7, s=60, color=colors[i])\n",
    "\n",
    "    ax1.set_xlabel('Sample Index')\n",
    "    ax1.set_ylabel('Baseline Entropy')\n",
    "    ax1.set_title('Baseline Uncertainty for Different Sentence Types')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Intervention effect comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    category_means = df.groupby('category')[['entropy_change_zero', 'entropy_change_mean']].mean()\n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "\n",
    "    ax2.bar(x - width/2, category_means['entropy_change_zero'], width,\n",
    "           label='Zero Intervention', alpha=0.8, color='red')\n",
    "    ax2.bar(x + width/2, category_means['entropy_change_mean'], width,\n",
    "           label='Mean Intervention', alpha=0.8, color='blue')\n",
    "\n",
    "    ax2.set_xlabel('Uncertainty Type')\n",
    "    ax2.set_ylabel('Average Entropy Change')\n",
    "    ax2.set_title('Comparison of Different Intervention Methods')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([cat.replace('_', '\\n') for cat in categories])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "    # 3. Scatter plot: baseline entropy vs intervention effect\n",
    "    ax3 = axes[1, 0]\n",
    "    for i, category in enumerate(categories):\n",
    "        cat_data = df[df['category'] == category]\n",
    "        ax3.scatter(cat_data['baseline_entropy'], cat_data['entropy_change_zero'],\n",
    "                   label=category.replace('_', ' '), alpha=0.7, s=60, color=colors[i])\n",
    "\n",
    "    ax3.set_xlabel('Baseline Entropy')\n",
    "    ax3.set_ylabel('Zero Intervention Entropy Change')\n",
    "    ax3.set_title('Baseline Uncertainty vs Intervention Effect')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "    # 4. Effect strength distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    all_changes = np.concatenate([df['entropy_change_zero'], df['entropy_change_mean']])\n",
    "    intervention_types = ['Zero Intervention'] * len(df) + ['Mean Intervention'] * len(df)\n",
    "\n",
    "    ax4.hist([df['entropy_change_zero'], df['entropy_change_mean']],\n",
    "            bins=10, alpha=0.7, label=['Zero Intervention', 'Mean Intervention'], color=['red', 'blue'])\n",
    "\n",
    "    ax4.set_xlabel('Entropy Change')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Distribution of Intervention Effect Strength')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate analysis charts\n",
    "plot_experiment_results(results_df)\n",
    "\n",
    "# === Experiment Conclusions ===\n",
    "print(\"=== ğŸ¯ Experiment Conclusions ===\")\n",
    "\n",
    "# Calculate overall effects\n",
    "overall_zero_effect = results_df['entropy_change_zero'].mean()\n",
    "overall_mean_effect = results_df['entropy_change_mean'].mean()\n",
    "\n",
    "print(f\"Overall intervention effects:\")\n",
    "print(f\"  Zero intervention average effect: {overall_zero_effect:.4f}\")\n",
    "print(f\"  Mean intervention average effect: {overall_mean_effect:.4f}\")\n",
    "\n",
    "# Statistics for significant effects\n",
    "significant_zero = (abs(results_df['entropy_change_zero']) > 0.1).sum()\n",
    "significant_mean = (abs(results_df['entropy_change_mean']) > 0.1).sum()\n",
    "total_samples = len(results_df)\n",
    "\n",
    "print(f\"\\nSignificant effect statistics (|change| > 0.1):\")\n",
    "print(f\"  Zero intervention significant effects: {significant_zero}/{total_samples} ({significant_zero/total_samples:.1%})\")\n",
    "print(f\"  Mean intervention significant effects: {significant_mean}/{total_samples} ({significant_mean/total_samples:.1%})\")\n",
    "\n",
    "# Uncertainty neuron determination\n",
    "if overall_zero_effect > 0.05 or overall_mean_effect > 0.05:\n",
    "    print(f\"\\nâœ… Conclusion: Neurons {test_neurons} may be uncertainty neurons!\")\n",
    "    print(\"   - Intervention on these neurons significantly affected model prediction uncertainty\")\n",
    "    print(\"   - Recommend conducting deeper analysis and testing more neurons\")\n",
    "else:\n",
    "    print(f\"\\nâ“ Conclusion: The uncertainty role of neurons {test_neurons} is not obvious\")\n",
    "    print(\"   - Recommend testing other candidate neurons\")\n",
    "    print(\"   - Or try different intervention methods\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Your first mechanistic interpretability experiment is complete!\")\n",
    "print(\"   Next steps you can try:\")\n",
    "print(\"   â€¢ Test neurons from more layers\")\n",
    "print(\"   â€¢ Use a larger test dataset\")\n",
    "print(\"   â€¢ Implement more refined activation patching methods\")\n",
    "print(\"   â€¢ Analyze specific types of uncertainty neurons\")\n",
    "\n",
    "# Display current memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nCurrent GPU memory usage: {torch.cuda.memory_allocated() // 1024**2} MB\")\n",
    "    print(f\"Peak GPU memory: {torch.cuda.max_memory_allocated() // 1024**2} MB\")\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOyt51BexA8L38qD5skep9m",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2f5248b2c94a41b09f8094344b80f306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_690b9b77f7b54bcbbfe246e01ab197de",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65d3426321e44edca52c6850f1494c79",
      "value": 3
     }
    },
    "4305d4c9e506498982f1358da9766115": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "51321c3847cd4de1802f420ae6363c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab4253bd986c4a6b948ea0c2d16427d9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4305d4c9e506498982f1358da9766115",
      "value": "â€‡3/3â€‡[00:03&lt;00:00,â€‡â€‡1.06it/s]"
     }
    },
    "618fd2b8b4f6446b97032145c448810a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65d3426321e44edca52c6850f1494c79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "690b9b77f7b54bcbbfe246e01ab197de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96d4a9152a4f4611a52895cdbe4045da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d1086d5889543f0839b4cd5ad7f4735": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3e0adbd96894b5eab19f097aaf31354",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_618fd2b8b4f6446b97032145c448810a",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "ab4253bd986c4a6b948ea0c2d16427d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af943e5689a94d99b4e14d9fab6fb441": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d1086d5889543f0839b4cd5ad7f4735",
       "IPY_MODEL_2f5248b2c94a41b09f8094344b80f306",
       "IPY_MODEL_51321c3847cd4de1802f420ae6363c33"
      ],
      "layout": "IPY_MODEL_96d4a9152a4f4611a52895cdbe4045da"
     }
    },
    "f3e0adbd96894b5eab19f097aaf31354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
